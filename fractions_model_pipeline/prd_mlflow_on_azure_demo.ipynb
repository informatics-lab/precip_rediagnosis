{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb682d2-0f94-43f9-8971-239a40ab19f5",
   "metadata": {},
   "source": [
    "# ML Flow on Azure ML\n",
    "\n",
    "The ML ops demo notebook shows running ML Flow on a local machine, and the AzureML notebook demonstrates using the Azure ML SDK for experiment tracking. This notebook combines the two, using AzureML to run, but tracking through the ML Flow API with AzureML providing the backend storage. This allows us to make use of the easily scaling  infrastructure of AzureML, while the code is still portable as other backends can easily be swapped in when required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "preliminary-idaho",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06959be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8300679e-340d-4fa3-9c7c-b23dc2ca9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40a4d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, concatenate\n",
    "from tensorflow.keras.layers import ZeroPadding1D, Reshape, Input, Dropout, PReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb927817-9259-4eda-b835-de060914651f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/08/23 11:26:03 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.tensorflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8be64f6-152c-4dc2-bdca-8bc8f292ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prd_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-investing",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b55f2dc-9d4e-49b4-9965-5d0a7d25d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from azureml.core import Experiment\n",
    "\n",
    "prd_ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ef9973-1466-4a42-b214-646ede49d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure_dataset_name ='sd3'\n",
    "# azure_experiment_name='prd_mlops_test'\n",
    "# azure_env_name = 'prd_ml_cluster'\n",
    "# cluster_name = 'mlops-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270e2027-fc87-4b05-8aea-f248982bbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prd_model_name = 'azml_mlflow_20220504'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85382878-1d22-4deb-92b9-58946c4bf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_parameter = [\n",
    "    'radar_fraction_in_band_instant_0.25', \n",
    "    'radar_fraction_in_band_instant_2.5',\n",
    "    'radar_fraction_in_band_instant_7.0',\n",
    "    'radar_fraction_in_band_instant_10.0'\n",
    "]\n",
    "profile_features = ['air_temperature', 'relative_humidity']\n",
    "single_lvl_features = [] #'air_pressure_at_sea_level'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b8d0cc4-2d8b-4b43-8d50-3314013e1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    'profile': profile_features,\n",
    "    'single_level': single_lvl_features,\n",
    "    'target': target_parameter,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1145f9a-f0aa-4f19-9589-f5aa85c2606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(prd_ws.get_mlflow_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca91538b-db49-4463-9886-67b4d5f36b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = prd_pipeline.load_data(\n",
    "#     prd_ws,\n",
    "#     dataset_name=azure_dataset_name\n",
    "# )\n",
    "# data_splits, data_dims = prd_pipeline.preprocess_data(\n",
    "#     input_data,\n",
    "#     test_fraction=0.2,\n",
    "#     feature_dict={'profile': profile_features, 'single_level': single_lvl_features,'target': target_parameter,},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ec17c2-0614-404c-b025-a5da877a60b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " PosixPath('/mnt/batch/tasks/shared/LS_root/mounts/clusters/prd-ml-fractions/code/Users/hannah.brown/precip_rediagnosis/data_prep/event_configs'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_dir = pathlib.Path.cwd().parent / 'data_prep' / 'event_configs'\n",
    "configs_dir.is_dir(), configs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c87cb4a0-77bb-4be5-bba0-a269077b10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path_list = [p1 for p1 in configs_dir.iterdir() if '20' in str(p1) and 'json' in str(p1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbfebd8c-8b26-4f0f-ac12-6ee6895a28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_merged_file_dataset_name = 'prd_merged_csv_files'\n",
    "train202208_dataset_all = azureml.core.Dataset.get_by_name(prd_ws, name=prd_merged_file_dataset_name)\n",
    "prd_prefix = 'prd'\n",
    "merged_prefix = prd_prefix + '_merged'\n",
    "csv_file_suffix = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad272875-286b-477d-bbbe-8bde48efe11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_name(dataset_config):\n",
    "\n",
    "    event_start_dt = datetime.datetime.strptime(dataset_config['event_start'], '%Y-%m-%dT%H:%MZ')\n",
    "    event_end_dt = datetime.datetime.strptime(dataset_config['event_end'], '%Y-%m-%dT%H:%MZ')\n",
    "    times_list = drivers.calc_dates_list(event_start_dt, event_end_dt, float(dataset_config['target_time_delta']))\n",
    "    final_timestamp = max(times_list)  # Data extract isn't inclusive of final date so need second last time \n",
    "\n",
    "    start_datestring = f'{event_start_dt.year:04d}{event_start_dt.month:02d}{event_start_dt.day:02d}T{event_start_dt.hour:02d}{event_start_dt.minute:02d}Z'\n",
    "    end_datestring = f'{event_end_dt.year:04d}{event_end_dt.month:02d}{event_end_dt.day:02d}T{event_end_dt.hour:02d}{event_end_dt.minute:02d}Z'\n",
    "    final_datestring = f'{final_timestamp.year:04d}{final_timestamp.month:02d}{final_timestamp.day:02d}T{final_timestamp.hour:02d}{final_timestamp.minute:02d}Z'\n",
    "    \n",
    "    return f'prd_merged_{start_datestring}_{final_datestring}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad9617b-b42f-476b-965c-180919e6f494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not mounting as a volume: ArgumentError(InvalidArgument { argument: \"arguments.path\", expected: \"Glob patterns inside the path are not supported by the volume mount.Path must be a direct path to the file or folder, or end with '/**' or '/**/*' to match the entire content of the volume.\", actual: \"REDACTED\" }). \n",
      "Falling back to dataflow mount.\n",
      "loading all data\n"
     ]
    }
   ],
   "source": [
    "with train202208_dataset_all.mount() as train202208_file_mount:\n",
    "    print('loading all data')\n",
    "    prd_path_list = [p1 for p1 in pathlib.Path(train202208_file_mount.mount_point).rglob(f'{merged_prefix}*{csv_file_suffix}') ]\n",
    "    merged_df = pd.concat([pd.read_csv(p1) for p1 in prd_path_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b72b7c70-b73a-4f90-ac6b-9e3671f6144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target has dims: 4\n",
      "dropping smallest bin: radar_fraction_in_band_instant_0.25\n",
      "getting profile columns\n",
      "{'profile': ['air_temperature', 'relative_humidity'], 'single_level': [], 'target': ['radar_fraction_in_band_instant_0.25', 'radar_fraction_in_band_instant_2.5', 'radar_fraction_in_band_instant_7.0', 'radar_fraction_in_band_instant_10.0']}\n"
     ]
    }
   ],
   "source": [
    "test_fraction=0.2\n",
    "df_train, df_test = prd_pipeline.random_time_space_sample(\n",
    "    merged_df, test_fraction=test_fraction, random_state=np.random.RandomState(), sampling_columns = ['time', 'latitude', 'longitude'])\n",
    "data_splits, data_dims_dict = prd_pipeline.preprocess_data(\n",
    "    df_train, feature_dict, test_fraction=test_fraction/(1-test_fraction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7def4473-cb5d-401a-9d6b-671682dbddae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7ccf9441-4968-4c88-9b08-154061256d80'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1 = mlflow.create_experiment('prd_fraction_models')\n",
    "exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8b3a87b-bdcd-49df-84c7-f37d8aa78004",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = mlflow.get_experiment(exp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a27fd2cd-ecda-4bee-82e3-13a6b876e3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='', experiment_id='7ccf9441-4968-4c88-9b08-154061256d80', lifecycle_stage='active', name='prd_fraction_models', tags={}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7243f14-bdd5-4348-ad0e-bf8729197580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = 'log/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91a2ab34-98b2-428a-ae55-4e04fda66df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:28:36.970461: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2022-08-23 11:28:36.971857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n"
     ]
    }
   ],
   "source": [
    "tensorflow_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# run tensorboard --logdir LOGDIRPATH from command line to launch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0669dad0-af4a-496e-83e4-280797bc2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20937b8a-77c4-4637-8605-1f784cec0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_dict = {\n",
    "    'loss_function': tf.keras.losses.KLDivergence(),\n",
    "    'epochs': 20, \n",
    "    'learning_rate': 0.001, \n",
    "    'batch_size': 500\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18e0343a-b343-4fc3-8cf9-17678d970b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 12:10:28.808615: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-23 12:10:28.808716: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (prd-ml-fractions): /proc/driver/nvidia/version does not exist\n",
      "2022-08-23 12:10:28.811184: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-08-23 12:10:28.878904: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2593905000 Hz\n",
      "2022-08-23 12:10:28.880543: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe8e4000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-08-23 12:10:28.880579: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model built\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "profile_input (InputLayer)      [(None, 33, 2)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d (ZeroPadding1D)  (None, 35, 2)        0           profile_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 33, 32)       192         zero_padding1d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_1 (ZeroPadding1D (None, 35, 32)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 33, 32)       3072        zero_padding1d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding1d_2 (ZeroPadding1D (None, 35, 32)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 33, 32)       3072        zero_padding1d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 33, 32)       0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1056)         0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 66)           69696       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            268         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 76,300\n",
      "Trainable params: 76,300\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "training_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 12:10:29.309437: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   6/1236 [..............................] - ETA: 30s - loss: 0.5842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 12:10:32.582683: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2022-08-23 12:10:32.632518: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32\n",
      "2022-08-23 12:10:32.642345: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32/prd-ml-fractions.trace.json.gz\n",
      "2022-08-23 12:10:32.656968: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.024 ms\n",
      "\n",
      "2022-08-23 12:10:32.674613: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32Dumped tool data for overview_page.pb to /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32/prd-ml-fractions.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32/prd-ml-fractions.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32/prd-ml-fractions.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /tmp/tmpkuym0dhr/train/plugins/profile/2022_08_23_12_10_32/prd-ml-fractions.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236/1236 [==============================] - 15s 12ms/step - loss: 0.3308 - val_loss: 0.3156\n",
      "Epoch 2/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.3126 - val_loss: 0.3098\n",
      "Epoch 3/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.3051 - val_loss: 0.3036\n",
      "Epoch 4/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.2989 - val_loss: 0.2989\n",
      "Epoch 5/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.2941 - val_loss: 0.2952\n",
      "Epoch 6/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.2899 - val_loss: 0.2920\n",
      "Epoch 7/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2864 - val_loss: 0.2915\n",
      "Epoch 8/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.2831 - val_loss: 0.2887\n",
      "Epoch 9/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2804 - val_loss: 0.2855\n",
      "Epoch 10/20\n",
      "1236/1236 [==============================] - 14s 11ms/step - loss: 0.2777 - val_loss: 0.2847\n",
      "Epoch 11/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2756 - val_loss: 0.2840\n",
      "Epoch 12/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2735 - val_loss: 0.2808\n",
      "Epoch 13/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2715 - val_loss: 0.2817\n",
      "Epoch 14/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2698 - val_loss: 0.2795\n",
      "Epoch 15/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2682 - val_loss: 0.2785\n",
      "Epoch 16/20\n",
      "1236/1236 [==============================] - 12s 10ms/step - loss: 0.2672 - val_loss: 0.2796\n",
      "Epoch 17/20\n",
      "1236/1236 [==============================] - 12s 10ms/step - loss: 0.2654 - val_loss: 0.2791\n",
      "Epoch 18/20\n",
      "1236/1236 [==============================] - 12s 10ms/step - loss: 0.2640 - val_loss: 0.2777\n",
      "Epoch 19/20\n",
      "1236/1236 [==============================] - 13s 11ms/step - loss: 0.2629 - val_loss: 0.2774\n",
      "Epoch 20/20\n",
      "1236/1236 [==============================] - 13s 10ms/step - loss: 0.2618 - val_loss: 0.2790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "2022/08/23 12:14:51 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: module 'tensorflow.compat.v2' has no attribute '__internal__'\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(experiment_id=exp1.experiment_id) as current_run:\n",
    "    print('starting')\n",
    "    model = prd_pipeline.build_model(**data_dims_dict)\n",
    "    print('model built')\n",
    "    model.summary()\n",
    "    print('training_model')\n",
    "    model = prd_pipeline.train_model(model, data_splits, hyperparameter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e739e218-21cf-4ae4-b357-5c226177002f",
   "metadata": {},
   "source": [
    "If we look at the experiment in AzureML GUI, we see that all the model parameters have been automatically logged, and the model has been saved by ML Flow ready for use in inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_hist_df = pd.DataFrame(history.history)\n",
    "# training_hist_df['epoch'] = history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ec093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.plot(training_hist_df.epoch, training_hist_df.loss, label='training')\n",
    "# plt.plot(training_hist_df.epoch, training_hist_df.val_loss, c='g', label='validation')\n",
    "# plt.legend()\n",
    "# plt.ylabel('MAE [mm of precipitation]')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.hist(data_splits['y_val'], alpha=0.5, bins=40, label='Actual')\n",
    "# plt.hist(y_pred, alpha=0.5, bins=40, label='Predicted')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197da54b-4b3d-45b0-8ba7-d91ca6839419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
